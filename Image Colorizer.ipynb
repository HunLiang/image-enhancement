{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Colorizer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lR4SIN6kU9VF"},"source":["#uncomment this line if you get import errors\n","!pip install tensorflowjs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S46fXu9OolIZ"},"source":["import glob\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow\n","from tensorflow import keras\n","from keras import models, layers\n","import tensorflowjs as tfjs\n","import zipfile\n","from zipfile import ZipFile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWSxboyhcTCi"},"source":["The following three code cells involve loading the images, converting them to the Lab and greyscale colorspaces, and dividing them into testing and training feature/label sets."]},{"cell_type":"code","metadata":{"id":"o7QgqWeVo7--"},"source":["#return the set of grayscale and color images from the given filepath\n","def getData(filepath):\n","  rgb_list = []\n","  gray_list = []\n","  file_path = glob.glob(os.path.join(filepath,'*.jpg'))\n","\n","  for img_path in file_path:\n","    rgb_img = cv2.imread(img_path) #take in the BGR image in the form of a numpy array\n","    gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY) #convert to grayscale using cv2\n","\n","    rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2LAB)\n","    rgb_img = cv2.resize(rgb_img,(224,224))\n","    gray_img = cv2.resize(gray_img,(224,224))\n","    rgb_list.append(rgb_img)\n","    gray_list.append(gray_img)\n","\n","  rgb_images = np.stack(rgb_list,axis=0)\n","  gray_images = np.stack(gray_list,axis=0)\n","  gray_images = np.reshape(gray_images,(len(gray_images),224,224,1))\n","\n","  #print(np.shape(rgb_images),np.shape(gray_images))\n","  return rgb_images , gray_images\n","\n","#combine L and ab channels into a single image\n","def combineLab(L, ab):\n","  outLab = np.empty((224, 224, 3))\n","  outLab[:, :, 1:3] = ab\n","  outLab[:, :, 0] = L.reshape(224, 224)\n","  #outLab[:, :, 0] = np.full((224, 224), .5)\n","  outLab = (outLab * 255).astype(np.uint8);\n","  return cv2.cvtColor(outLab, cv2.COLOR_LAB2RGB)\n","\n","#resize image\n","def resize(filepath):\n","  in_path = glob.glob(os.path.join(filepath,'*.jpg'))\n","  for i in range(0, len(in_path)):\n","    rgb_img = cv2.imread(in_path[i]) #take in the BGR image in the form of a numpy array\n","    rgb_img = cv2.resize(rgb_img,(224,224))\n","    cv2.imwrite(in_path[i], rgb_img)\n","\n","#resize(\"drive/MyDrive/ML/landscapes_some\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SjZzv6_emNn"},"source":["with ZipFile('landscapes2000.zip', 'r') as zipObj:\n","  zipObj.extractall('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4s4fy_33rhnB"},"source":["#prepare features and labels (inputs and outputs, respectively)\n","training_labels, training_features = getData(\"training\")\n","training_labels = training_labels / 255.\n","training_features = training_features / 255.;\n","\n","testing_labels, testing_features = getData(\"testing\")\n","testing_labels = testing_labels / 255.\n","testing_features = testing_features / 255.;\n","\n","#chop the L component off the Lab image to leave just the ab components\n","training_labels = training_labels[:, :, :, 1:3]\n","testing_labels = testing_labels[:, :, :, 1:3]\n","\n","training_labels.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwa7ifQqcmfv"},"source":["The following cells set up the model, train it, evaluate it using testing data, and show a sample with one image in the testing data."]},{"cell_type":"code","metadata":{"id":"U1DYMXmXvQdF"},"source":["#create the model used for colorizing images\n","input_layer = layers.Input(shape=(224,224,1))\n","encoder_layer = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(input_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(128, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(256, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","\n","decoder_layer = layers.UpSampling2D((2, 2))(encoder_layer)\n","decoder_layer = layers.Conv2D(128, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(16, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","output_layer = layers.Conv2D(2, (3, 3), padding = 'same', activation = 'sigmoid')(decoder_layer)\n","\n","model = models.Model(input_layer, output_layer)\n","model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eu9ItDXix89x"},"source":["#train and save the model.\n","#If desired, the top two lines can be commented out and the 3rd line \n","#uncommented to load an existing model, as opposed to training a new model.\n","model.fit(training_features, training_labels, batch_size=32, epochs=10)\n","model.save('landscapes.keras')\n","tfjs.converters.save_keras_model(model, 'landscapes.json') #btw, I added this line for exporting the model to js. Comment this out if you're loading an existing model\n","#model = models.load_model('landscapes35.keras')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7NTUl8kjxp2"},"source":["#evaluate the model with both the training and testing data\n","model.evaluate(training_features, training_labels)\n","model.evaluate(testing_features, testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWx8H-B63K5F"},"source":["#test out the model with a specific image (at index n)\n","n = 28\n","greyscale = testing_features[n]\n","out = model.predict(testing_features[n].reshape(1, 224, 224, 1))\n","original = testing_labels[n]\n","\n","plt.figure(figsize=(12, 4), dpi=100)\n","plt.subplot(1, 3, 1)\n","plt.imshow(greyscale.reshape(224, 224), cmap='gray')\n","plt.subplot(1, 3, 2)\n","\n","plt.imshow(combineLab(greyscale, out))\n","plt.subplot(1, 3, 3)\n","plt.imshow(combineLab(greyscale, original))"],"execution_count":null,"outputs":[]}]}