{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Colorizer (Disk).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lR4SIN6kU9VF"},"source":["#uncomment this line if you get import errors\n","!pip install tensorflowjs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S46fXu9OolIZ"},"source":["import glob\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow\n","from tensorflow import keras\n","from keras import models, layers\n","import tensorflowjs as tfjs\n","import zipfile\n","from zipfile import ZipFile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWSxboyhcTCi"},"source":["The following three code cells involve loading the images, converting them to the Lab and greyscale colorspaces, and dividing them into testing and training feature/label sets."]},{"cell_type":"code","metadata":{"id":"o7QgqWeVo7--"},"source":["def getColorBWImages(filepath):\n","  rgb_img = cv2.imread(filepath) #take in the BGR image in the form of a numpy array\n","  gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY) #convert to grayscale using cv2\n","  lab_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2LAB)\n","  #rgb_img = cv2.resize(rgb_img,(224,224))\n","  #gray_img = cv2.resize(gray_img,(224,224))\n","  gray_img = gray_img.reshape(224, 224, 1) / 255.; #TODO: check\n","  lab_img = lab_img[:, :, 1:3] / 255.\n","  return gray_img, lab_img #gray image has shape (x, y), ab image has shape (x, y, 2)\n","\n","#combine L and ab channels into a single image\n","def combineLab(L, ab):\n","  outLab = np.empty((224, 224, 3))\n","  outLab[:, :, 1:3] = ab\n","  outLab[:, :, 0] = L.reshape(224, 224)\n","  #outLab[:, :, 0] = np.full((224, 224), .5)\n","  outLab = (outLab * 255).astype(np.uint8);\n","  return cv2.cvtColor(outLab, cv2.COLOR_LAB2RGB)\n","\n","#resize image\n","def resize(filepath):\n","  in_path = glob.glob(os.path.join(filepath,'*.jpg'))\n","  for i in range(0, len(in_path)):\n","    rgb_img = cv2.imread(in_path[i]) #take in the BGR image in the form of a numpy array\n","    rgb_img = cv2.resize(rgb_img,(224,224))\n","    cv2.imwrite(in_path[i], rgb_img)\n","\n","#resize(\"drive/MyDrive/ML/landscapes_some\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SjZzv6_emNn"},"source":["with ZipFile('landscapes2000.zip', 'r') as zipObj:\n","  zipObj.extractall('')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training data uses a custom data generator to load data from the disk during training. Testing data isn't that large, so the old system is used and data is loaded into RAM all at once."],"metadata":{"id":"JQBI_MSAMhYG"}},{"cell_type":"code","source":["features = dict()\n","features['training'] = list()\n","features['testing'] = list()\n","\n","training_path = glob.glob(os.path.join(\"training\",'*.jpg'))\n","testing_path = glob.glob(os.path.join(\"testing\",'*.jpg'))\n","\n","for path in training_path:\n","    features['training'].append(path);\n","for path in testing_path:\n","    features['testing'].append(path);"],"metadata":{"id":"gYL9bgO-IS45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data generator template provided by https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n","class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, batch_size=64, shuffle=True):\n","        'Initialization'\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","            \n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples'\n","        # Initialization\n","        X = np.empty((self.batch_size, 224, 224, 1))\n","        y = np.empty((self.batch_size, 224, 224, 2))\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            gray_img, lab_img = getColorBWImages(ID);\n","            X[i] = gray_img\n","            y[i] = lab_img\n","\n","        return X, y"],"metadata":{"id":"5xt25WC1M0hM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwa7ifQqcmfv"},"source":["The following cells set up the model, train it, evaluate it using testing data, and show a sample with one image in the testing data."]},{"cell_type":"code","metadata":{"id":"U1DYMXmXvQdF","collapsed":true},"source":["#create the model used for colorizing images\n","input_layer = layers.Input(shape=(224,224,1))\n","encoder_layer = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(input_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(128, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","encoder_layer = layers.Conv2D(256, (3, 3), activation = 'relu', padding = 'same')(encoder_layer)\n","encoder_layer = layers.MaxPooling2D((2, 2), padding='same')(encoder_layer)\n","\n","decoder_layer = layers.UpSampling2D((2, 2))(encoder_layer)\n","decoder_layer = layers.Conv2D(128, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","decoder_layer = layers.UpSampling2D((2, 2))(decoder_layer)\n","decoder_layer = layers.Conv2D(16, (3, 3), activation = 'relu', padding = 'same')(decoder_layer)\n","output_layer = layers.Conv2D(2, (3, 3), padding = 'same', activation = 'sigmoid')(decoder_layer)\n","\n","model = models.Model(input_layer, output_layer)\n","model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n","#model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_generator = DataGenerator(features['training'])\n","validation_generator = DataGenerator(features['testing'])\n","\n","model.fit(training_generator, epochs=18)\n","model.save('landscapes.keras')\n","tfjs.converters.save_keras_model(model, 'landscapes.json') #btw, I added this line for exporting the model to js. Comment this out if you're loading an existing model"],"metadata":{"id":"2pWe_qgvq4rA"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7NTUl8kjxp2"},"source":["#evaluate the model with both the training and testing data\n","model.evaluate(training_generator)\n","model.evaluate(validation_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWx8H-B63K5F"},"source":["#test out the model with a specific image (at index n)\n","n = 16\n","gray_img, lab_img = getColorBWImages(features['testing'][n])\n","\n","out = model.predict(gray_img.reshape(1, 224, 224, 1))\n","\n","plt.figure(figsize=(12, 4), dpi=100)\n","plt.subplot(1, 3, 1)\n","plt.imshow(gray_img.reshape(224, 224), cmap='gray')\n","plt.subplot(1, 3, 2)\n","\n","plt.imshow(combineLab(gray_img, out))\n","plt.subplot(1, 3, 3)\n","plt.imshow(combineLab(gray_img, lab_img))"],"execution_count":null,"outputs":[]}]}